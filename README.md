# HW4-Reinforcement-Learning

Group Members:
- Michael Grajera
- Chase Mortensen
- Anthony Wilson

## Running the program

In order to run the program, install the following packages: `matplotlib`, `gym`, `gymnasium`, `numpy`, `torch`, and `stable-baselines3`.

Then, run

```sh
python main.py
```

## Record the average reward as a baseline for later comparison.

```
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 23.2     |
| time/              |          |
|    fps             | 1552     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 28.2          |
|    ep_rew_mean          | 28.2          |
| time/                   |               |
|    fps                  | 1039          |
|    iterations           | 2             |
|    time_elapsed         | 3             |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.008430149   |
|    clip_fraction        | 0.088         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.687        |
|    explained_variance   | -0.0048627853 |
|    learning_rate        | 0.0003        |
|    loss                 | 7.79          |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.0131       |
|    value_loss           | 57.5          |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.8         |
|    ep_rew_mean          | 34.8         |
| time/                   |              |
|    fps                  | 944          |
|    iterations           | 3            |
|    time_elapsed         | 6            |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0096760895 |
|    clip_fraction        | 0.087        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.11428839   |
|    learning_rate        | 0.0003       |
|    loss                 | 14.9         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0207      |
|    value_loss           | 34.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.2        |
|    ep_rew_mean          | 46.2        |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008846871 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.641      |
|    explained_variance   | 0.21914816  |
|    learning_rate        | 0.0003      |
|    loss                 | 27          |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 56.1        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 62           |
|    ep_rew_mean          | 62           |
| time/                   |              |
|    fps                  | 896          |
|    iterations           | 5            |
|    time_elapsed         | 11           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0076143593 |
|    clip_fraction        | 0.0706       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.27840477   |
|    learning_rate        | 0.0003       |
|    loss                 | 19.3         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0179      |
|    value_loss           | 63.3         |
------------------------------------------
Baseline Average Reward: 156.7
```

## Compare the baseline and modified agentâ€™s performance. How did changing the reward function affect behavior?

Baseline Average Reward: 156.7
Average reward with custom reward function: 165.9

The average rewards have been pretty similar. Both seem to vary quite a bit each time we run the program. The custom reward function seems to end up with a higher reward more often than the baseline reward function, but it depends on the run and they are often pretty close.

While the improvement is modest, it demonstrates that customizing the reward structure can encourage different behaviors, such as better balance and upright positioning. However, the results showed some variability across runs, with rewards occasionally being close to the baseline.

Here's the full output from one of the modified agent's runs:

```
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 5078     |
|    iterations      | 1        |
|    time_elapsed    | 0        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.1         |
|    ep_rew_mean          | 25.9         |
| time/                   |              |
|    fps                  | 3552         |
|    iterations           | 2            |
|    time_elapsed         | 1            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.009991797  |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.685       |
|    explained_variance   | -0.009401798 |
|    learning_rate        | 0.0003       |
|    loss                 | 5.63         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.02        |
|    value_loss           | 44.8         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.4         |
|    ep_rew_mean          | 32.6         |
| time/                   |              |
|    fps                  | 3266         |
|    iterations           | 3            |
|    time_elapsed         | 1            |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0096580675 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.659       |
|    explained_variance   | 0.085132     |
|    learning_rate        | 0.0003       |
|    loss                 | 12.8         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0169      |
|    value_loss           | 32.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.5        |
|    ep_rew_mean          | 42.9        |
| time/                   |             |
|    fps                  | 3141        |
|    iterations           | 4           |
|    time_elapsed         | 2           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008621851 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.20792335  |
|    learning_rate        | 0.0003      |
|    loss                 | 17.6        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 46.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 59.1        |
|    ep_rew_mean          | 54.3        |
| time/                   |             |
|    fps                  | 3068        |
|    iterations           | 5           |
|    time_elapsed         | 3           |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.006489607 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.35760826  |
|    learning_rate        | 0.0003      |
|    loss                 | 18.6        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 51.7        |
-----------------------------------------
Average reward with custom reward function: 173.52622567371105
```
## Compare the baseline and custom neural network architecture's performance. How did changing the architecture affect behavior?

The custom neural network architecture significantly enhanced the agent's performance, achieving an average reward of 200.1, which is notably higher than both the baseline reward and the reward with the custom reward function. This improvement highlights the impact of increasing model complexity and tailoring the network's structure to better capture the environment's dynamics.

```
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 1554     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.2        |
|    ep_rew_mean          | 27.2        |
| time/                   |             |
|    fps                  | 1070        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011969291 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.0182      |
|    learning_rate        | 0.0003      |
|    loss                 | 8.16        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 51          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37          |
|    ep_rew_mean          | 37          |
| time/                   |             |
|    fps                  | 1076        |
|    iterations           | 3           |
|    time_elapsed         | 5           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.009763462 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.0606      |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 36.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 51.8        |
|    ep_rew_mean          | 51.8        |
| time/                   |             |
|    fps                  | 1067        |
|    iterations           | 4           |
|    time_elapsed         | 7           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008619969 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.195       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 57.3        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 67.2        |
|    ep_rew_mean          | 67.2        |
| time/                   |             |
|    fps                  | 1069        |
|    iterations           | 5           |
|    time_elapsed         | 9           |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.007016697 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0003      |
|    loss                 | 17          |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 55.7        |
-----------------------------------------
Average reward with custom network architecture: 200.1
```

## Overall impact of the modifications
![image info](./images/image.png)

| Configuration            | Average Reward |
|--------------------------|----------------|
| Baseline Reward          | 156.7          |
| Custom Reward Function   | 165.9          |
| Custom Neural Network    | 200.1          |

The custom reward function provided a moderate improvement over the baseline, demonstrating that tuning the reward can guide the agent toward desirable behaviors.

The custom neural network architecture had a more substantial impact, yielding the highest average reward and showcasing the importance of model design in reinforcement learning tasks.

The variability in results across runs emphasizes the stochastic nature of reinforcement learning, suggesting the need for multiple trials to ensure robust evaluation.